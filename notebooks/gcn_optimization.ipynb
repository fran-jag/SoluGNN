{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Graph Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdmolops, rdDistGeom\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to transform SMILE into graph\n",
    "\n",
    "class Graph():\n",
    "\n",
    "    def __init__(self,\n",
    "                molecule_smiles: str,\n",
    "                node_vec_len: int,\n",
    "                max_atoms: int = None,\n",
    "                ) -> None:\n",
    "        \n",
    "        # Store properties\n",
    "        self.smiles = molecule_smiles\n",
    "        self.node_vec_len = node_vec_len\n",
    "        self.max_atoms = max_atoms\n",
    "\n",
    "        # Convert SMILES to RDKit mol\n",
    "        self.smiles_to_mol()\n",
    "\n",
    "        # Check if valid mol was created and generate graph\n",
    "        if self.mol:\n",
    "            self.smiles_to_graph()\n",
    "    \n",
    "    def smiles_to_mol(self):\n",
    "        \n",
    "        mol = Chem.MolFromSmiles(self.smiles)\n",
    "\n",
    "        if mol is None:\n",
    "            self.mol = None\n",
    "            return\n",
    "        \n",
    "        self.mol = Chem.AddHs(mol)\n",
    "\n",
    "    def smiles_to_graph(self):\n",
    "        \n",
    "        # Get list of atoms in molecule\n",
    "        atoms = self.mol.GetAtoms()\n",
    "\n",
    "        # If max_atoms is not provided, max_atoms = len(atoms)\n",
    "        if self.max_atoms is None:\n",
    "            n_atoms = len(list(atoms))\n",
    "        else:\n",
    "            n_atoms = self.max_atoms\n",
    "        \n",
    "        # Create empty node matrix\n",
    "        node_mat = np.zeros((n_atoms, self.node_vec_len))\n",
    "\n",
    "        # Iterate over atoms and add note to matrix\n",
    "        for atom in atoms:\n",
    "            # Get atom index and atomic number\n",
    "            atom_index = atom.GetIdx()\n",
    "            atom_no = atom.GetAtomicNum()\n",
    "\n",
    "            # Assign to node matrix\n",
    "            node_mat[atom_index, atom_no] = 1\n",
    "        \n",
    "        # Get adjacency matrix using RDKit\n",
    "        adj_mat = rdmolops.GetAdjacencyMatrix(self.mol)\n",
    "        self.std_adj_mat = np.copy(adj_mat)\n",
    "\n",
    "        # Get ditance matrix using RDKit\n",
    "        dist_mat = rdDistGeom.GetMoleculeBoundsMatrix(self.mol)\n",
    "        dist_mat[dist_mat == 0.] = 1 # Avoids division by 0\n",
    "\n",
    "        # Get modified adjacency matrix with inverse bond lengths\n",
    "        adj_mat = adj_mat * (1 / dist_mat)\n",
    "\n",
    "        # Par the adjacency matrix with 0s\n",
    "        dim_add = n_atoms - adj_mat.shape[0]\n",
    "        adj_mat = np.pad(\n",
    "            adj_mat,\n",
    "            pad_width=((0, dim_add), (0, dim_add)),\n",
    "            mode='constant',\n",
    "        )\n",
    "\n",
    "        # Add an identity matrix to adjacency matrix,\n",
    "        # this makes an atom its own neighbor\n",
    "        adj_mat = adj_mat + np.eye(n_atoms)\n",
    "\n",
    "        # Save adjacency and node matrices\n",
    "        self.node_mat = node_mat\n",
    "        self.adj_mat = adj_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pytorch dataset class\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset_path: str,\n",
    "                 node_vec_len: int,\n",
    "                 max_atoms: int) -> None:\n",
    "        self.node_vec_len = node_vec_len\n",
    "        self.max_atoms = max_atoms\n",
    "\n",
    "        # Open dataset file\n",
    "        df = pd.read_csv(dataset_path)\n",
    "\n",
    "        # Create lists\n",
    "        self.indices = df.index.to_list()\n",
    "        self.smiles = df['smiles'].to_list()\n",
    "        self.outputs = df['expt'].to_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, i: int):\n",
    "        # Get SMILE\n",
    "        smile = self.smiles[i]\n",
    "\n",
    "        # Create graph using the Graph class\n",
    "        mol = Graph(smile,\n",
    "                    self.node_vec_len,\n",
    "                    self.max_atoms)\n",
    "        \n",
    "        # Get the matrices\n",
    "        node_mat = torch.Tensor(mol.node_mat)\n",
    "        adj_mat = torch.Tensor(mol.adj_mat)\n",
    "\n",
    "        # Get output\n",
    "        output = torch.Tensor([self.outputs[i]])\n",
    "\n",
    "        return (node_mat, adj_mat), output, smile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom collate function for DataLoader\n",
    "def collate_graph_dataset(dataset: Dataset):\n",
    "    # Create empty lists\n",
    "    node_mats = []\n",
    "    adj_mats = []\n",
    "    outputs = []\n",
    "    smiles = []\n",
    "\n",
    "    # iterate over the dataset and assign components to the correct list\n",
    "    for i in range(len(dataset)):\n",
    "        (node_mat, adj_mat), output, smile = dataset[i]\n",
    "        node_mats.append(node_mat)\n",
    "        adj_mats.append(adj_mat)\n",
    "        outputs.append(output)\n",
    "        smiles.append(smile)\n",
    "\n",
    "    # Create tensors\n",
    "    node_mats_tensor = torch.cat(node_mats, dim=0)\n",
    "    adj_mats_tensor = torch.cat(adj_mats, dim=0)\n",
    "    outputs_tensor = torch.stack(outputs, dim=0)\n",
    "\n",
    "    # Return tensors\n",
    "    return (node_mats_tensor, adj_mats_tensor), outputs_tensor, smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of convolution layer\n",
    "\n",
    "class Convolutionlayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Note:\n",
    "    The ConvolutionLayer essentially does three things \n",
    "    - Calculation of the inverse diagonal degree matrix from the adjacency matrix\n",
    "    - Multiplication of the four matrices (D⁻¹ANW)\n",
    "    - Application of a non-linear activation function to the layer output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 node_in_len: int,\n",
    "                 node_out_len: int,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Create linear layer for node matrix\n",
    "        self.conv_linear = nn.Linear(node_in_len, node_out_len)\n",
    "\n",
    "        # Create activation function\n",
    "        self.conv_activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self,\n",
    "                node_mat,\n",
    "                adj_mat,\n",
    "                ):\n",
    "        # Calculate number of neighbors\n",
    "        n_neighbors = adj_mat.sum(dim=-1, keepdims=True)\n",
    "\n",
    "        # Create identity tensor\n",
    "        self.idx_mat = torch.eye(\n",
    "            adj_mat.shape[-2],\n",
    "            adj_mat.shape[-1],\n",
    "            device=n_neighbors.device\n",
    "        )\n",
    "\n",
    "        # Add new (batch) dimension and expand\n",
    "        idx_mat = self.idx_mat.unsqueeze(0).expand(*adj_mat.shape)\n",
    "        # Get inverse degree matrix\n",
    "        inv_degree_mat = torch.mul(idx_mat, 1 / n_neighbors)\n",
    "\n",
    "        # Perform matrix multiplication (D⁻¹AN)\n",
    "        node_fea = torch.bmm(inv_degree_mat, adj_mat)\n",
    "        node_fea = torch.bmm(node_fea, node_mat)\n",
    "\n",
    "        # Perfom linear transformation to node features (node_fea * W)\n",
    "        node_fea = self.conv_linear(node_fea)\n",
    "\n",
    "        # Apply activation\n",
    "        node_fea = self.conv_activation(node_fea)\n",
    "\n",
    "        return node_fea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of pooling layer\n",
    "\n",
    "class PoolingLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,\n",
    "                node_fea):\n",
    "        # Pool the node matrix\n",
    "        pooled_node_fea = node_fea.mean(dim=1)\n",
    "        return pooled_node_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Graph Neural Network\n",
    "\n",
    "class ChemGCN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            node_vec_len: int,\n",
    "            node_fea_len: int,\n",
    "            hidden_fea_len: int,\n",
    "            n_conv: int,\n",
    "            n_hidden: int,\n",
    "            n_outputs: int,\n",
    "            p_dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define layers\n",
    "        # Initial transformation from node matrix to node features\n",
    "        self.init_transform = nn.Linear(node_vec_len, node_fea_len)\n",
    "        \n",
    "        # Convolution layers\n",
    "        self.conv_layers = nn.ModuleList(\n",
    "            [Convolutionlayer(node_in_len=node_fea_len,\n",
    "                             node_out_len=node_fea_len,\n",
    "                             )\n",
    "                             for i in range(n_conv)]\n",
    "        )\n",
    "\n",
    "        # Pool convolution outputs\n",
    "        self.pooling = PoolingLayer()\n",
    "        pooled_node_fea_len = node_fea_len\n",
    "\n",
    "        # Pooling activation\n",
    "        self.pooling_activation = nn.LeakyReLU()\n",
    "\n",
    "        # From pooling layers to hidden layers\n",
    "        self.pooled_to_hidden = nn.Linear(pooled_node_fea_len, hidden_fea_len)\n",
    "\n",
    "        # Hidden layer\n",
    "        self.hidden_layer = nn.Linear(hidden_fea_len, hidden_fea_len)\n",
    "\n",
    "        # Hidden layer activation function\n",
    "        self.hidden_activation = nn.LeakyReLU()\n",
    "\n",
    "        # Hidden layer dropout\n",
    "        self.dropout = nn.Dropout(p=p_dropout)\n",
    "\n",
    "        # If hidden layer > 1, add more hidden layers\n",
    "        self.n_hidden = n_hidden\n",
    "        if self.n_hidden > 1:\n",
    "            self.hidden_layers = nn.ModuleList(\n",
    "                [self.hidden_layer for _ in range(n_hidden -1)]\n",
    "            )\n",
    "            self.hidden_activation_layers = nn.ModuleList(\n",
    "                [self.hidden_activation for _ in range(n_hidden - 1)]\n",
    "                )\n",
    "            self.hidden_dropout_layers = nn.ModuleList(\n",
    "                [self.dropout for _ in range(n_hidden - 1)]\n",
    "            )\n",
    "        \n",
    "        # Final layer going to output\n",
    "        self. hidden_to_output = nn.Linear(hidden_fea_len, n_outputs)\n",
    "\n",
    "    def forward(self, node_mat, adj_mat):\n",
    "        # Perform initial transform on node_mat\n",
    "        node_fea = self.init_transform(node_mat)\n",
    "\n",
    "        # Perform convolutions\n",
    "        for conv in self.conv_layers:\n",
    "            node_fea = conv(node_fea, adj_mat)\n",
    "        \n",
    "        # Perform pooling\n",
    "        pooled_node_fea = self.pooling(node_fea)\n",
    "        pooled_node_fea = self.pooling_activation(pooled_node_fea)\n",
    "\n",
    "        # First hidden layer\n",
    "        hidden_node_fea = self.pooled_to_hidden(pooled_node_fea)\n",
    "        hidden_node_fea = self.hidden_activation(hidden_node_fea)\n",
    "        hidden_node_fea = self.dropout(hidden_node_fea)\n",
    "\n",
    "        # Subsequent hidden layer\n",
    "        if self.n_hidden > 1:\n",
    "            for i in range(self.n_hidden -1):\n",
    "                hidden_node_fea = self.hidden_layers[i](hidden_node_fea)\n",
    "                hidden_node_fea = self.hidden_activation_layers[i](hidden_node_fea)\n",
    "                hidden_node_fea = self.hidden_dropout_layers[i](hidden_node_fea)\n",
    "        # Output\n",
    "        out = self.hidden_to_output(hidden_node_fea)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standardizer\n",
    "\n",
    "class Standardizer:\n",
    "    def __init__(self, X):\n",
    "        self.mean = torch.mean(X)\n",
    "        self.std = torch.std(X)\n",
    "    \n",
    "    def standardize(self, X):\n",
    "        Z = (X - self.mean) / self.std\n",
    "        return Z\n",
    "    \n",
    "    def restore(self, Z):\n",
    "        X = self.mean + Z * self.std\n",
    "        return X\n",
    "    \n",
    "    def state(self):\n",
    "        return {'mean': self.mean, 'std': self.std}\n",
    "    \n",
    "    def load(self, state):\n",
    "        self.mean = state['mean']\n",
    "        self.std = state['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test functions\n",
    "\n",
    "def train_model(\n",
    "        epoch,\n",
    "        model,\n",
    "        training_dataloader,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        standardizer,\n",
    "        use_GPU,\n",
    "        max_atoms,\n",
    "        node_vec_len,\n",
    "):\n",
    "    # Variables to store losses and error\n",
    "    avg_loss = 0\n",
    "    avg_mae = 0\n",
    "    count = 0\n",
    "\n",
    "    # Switch model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # Go over each batch\n",
    "    for i, dataset in enumerate(training_dataloader):\n",
    "        # Unpack data\n",
    "        node_mat = dataset[0][0]\n",
    "        adj_mat = dataset[0][1]\n",
    "        output = dataset[1]\n",
    "\n",
    "        # Reshape inputs\n",
    "        first_dim = int((torch.numel(node_mat)) / (max_atoms * node_vec_len))\n",
    "        node_mat = node_mat.reshape(first_dim, max_atoms, node_vec_len)\n",
    "        adj_mat = adj_mat.reshape(first_dim, max_atoms, max_atoms)\n",
    "\n",
    "        # Standardize output\n",
    "        output_std = standardizer.standardize(output)\n",
    "\n",
    "        # Package inputs, outputs; check GPU\n",
    "        if use_GPU:\n",
    "            nn_input = (node_mat.cuda(), adj_mat.cuda())\n",
    "            nn_output = output_std.cuda()\n",
    "        else:\n",
    "            nn_input = (node_mat, adj_mat)\n",
    "            nn_output = output_std\n",
    "        \n",
    "        # Compute output from network\n",
    "        nn_prediction = model(*nn_input)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(nn_output, nn_prediction)\n",
    "        avg_loss += loss\n",
    "\n",
    "        # Calculate MAE\n",
    "        prediction = standardizer.restore(nn_prediction.detach().cpu())\n",
    "        mae = mean_absolute_error(output, prediction)\n",
    "        avg_mae += mae\n",
    "\n",
    "        # Set zero gradients for all tensors\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Do backward propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Increase count\n",
    "        count += 1\n",
    "    \n",
    "    # Calculate avg loss and MAE\n",
    "    avg_loss = avg_loss / count\n",
    "    avg_mae = avg_mae / count\n",
    "\n",
    "    # # Print stats\n",
    "    # if epoch % 10 == 0:\n",
    "    #     print(\n",
    "    #         'Epoch: [{0}]\\tTraining Loss: [{1:.2f}]\\tTraining MAE: [{2:.2f}]'\\\n",
    "    #         .format(\n",
    "    #             epoch, avg_loss, avg_mae\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    # Return loss and MAE\n",
    "    return avg_loss, avg_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test function\n",
    "\n",
    "def test_model(\n",
    "        model,\n",
    "        test_dataloader,\n",
    "        loss_fn,\n",
    "        standardizer,\n",
    "        use_GPU,\n",
    "        max_atoms,\n",
    "        node_vec_len,):\n",
    "    \n",
    "    # Store loss and error\n",
    "    test_loss = 0\n",
    "    test_mae = 0\n",
    "    count = 0\n",
    "    \n",
    "    # Store all outputs and predictions for R2\n",
    "    all_outputs = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Switch to inference mode\n",
    "    model.eval()\n",
    "\n",
    "    # Go over batches of test set\n",
    "    for i, dataset in enumerate(test_dataloader):\n",
    "        # Unpack data\n",
    "        node_mat = dataset[0][0]\n",
    "        adj_mat = dataset[0][1]\n",
    "        output = dataset[1]\n",
    "\n",
    "        # Reshape\n",
    "        first_dim = int((torch.numel(node_mat)) / (max_atoms * node_vec_len))\n",
    "        node_mat = node_mat.reshape(first_dim, max_atoms, node_vec_len)\n",
    "        adj_mat = adj_mat.reshape(first_dim, max_atoms, max_atoms)\n",
    "\n",
    "        # Standardize output\n",
    "        output_std = standardizer.standardize(output)\n",
    "\n",
    "        # Package inputs and outputs to GPU\n",
    "        if use_GPU:\n",
    "            nn_input = (node_mat.cuda(), adj_mat.cuda())\n",
    "            nn_output = output_std.cuda()\n",
    "        else:\n",
    "            nn_input = (node_mat, adj_mat)\n",
    "            nn_output = output_std\n",
    "        \n",
    "        # Compute output\n",
    "        nn_prediction = model(*nn_input)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(nn_output, nn_prediction)\n",
    "        test_loss += loss\n",
    "\n",
    "        # Calculate MAE\n",
    "        prediction = standardizer.restore(nn_prediction.detach().cpu())\n",
    "        mae = mean_absolute_error(output, prediction)\n",
    "        test_mae += mae\n",
    "\n",
    "        # Store predictions and actual values for R²\n",
    "        all_predictions.extend(prediction.numpy().flatten())\n",
    "        all_outputs.extend(output.numpy().flatten())\n",
    "\n",
    "\n",
    "        # Increase count\n",
    "        count += 1\n",
    "\n",
    "    # Calculate avg loss, MAE, R2\n",
    "    test_loss = test_loss / count\n",
    "    test_mae = test_mae / count\n",
    "    test_rmse = root_mean_squared_error(all_outputs, all_predictions)\n",
    "\n",
    "    return test_loss, test_mae, test_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow\n",
    "\n",
    "## Fix Seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "use_GPU = torch.cuda.is_available()\n",
    "\n",
    "## Inputs\n",
    "max_atoms = 100\n",
    "node_vec_len = 60\n",
    "train_size = 0.7\n",
    "batch_size = 32\n",
    "hidden_nodes = 60\n",
    "n_conv_layers = 4\n",
    "n_hidden_layers = 2\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "\n",
    "## Create dataset\n",
    "main_path = Path().resolve().parent\n",
    "data_path = main_path / 'data' / 'train.csv'\n",
    "dataset = GraphDataset(dataset_path=data_path,\n",
    "                       max_atoms=max_atoms,\n",
    "                       node_vec_len=node_vec_len)\n",
    "\n",
    "## Split data into train and test\n",
    "# Get sizes\n",
    "dataset_indices = np.arange(0, len(dataset), 1)\n",
    "train_size = int(np.round(train_size * len(dataset)))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Randomly sample train and test indices\n",
    "train_indices = np.random.choice(dataset_indices,\n",
    "                                 size=train_size,\n",
    "                                 replace=False)\n",
    "test_indices = np.array(list(set(dataset_indices) - set(train_indices)))\n",
    "\n",
    "# Create dataloaders\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "train_loader = DataLoader(dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          sampler=train_sampler,\n",
    "                          collate_fn=collate_graph_dataset)\n",
    "test_loader = DataLoader(dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         sampler=test_sampler,\n",
    "                         collate_fn=collate_graph_dataset)\n",
    "\n",
    "## Initialize model, standardizer, optimizer and loss functions\n",
    "# Model\n",
    "model = ChemGCN(node_vec_len=node_vec_len,\n",
    "                node_fea_len=hidden_nodes,\n",
    "                hidden_fea_len=hidden_nodes,\n",
    "                n_conv=n_conv_layers,\n",
    "                n_hidden=n_hidden_layers,\n",
    "                n_outputs=1,\n",
    "                p_dropout=0.1)\n",
    "# Transfer to GPU\n",
    "if use_GPU:\n",
    "    model.cuda()\n",
    "\n",
    "# Standardizer\n",
    "outputs = [dataset[i][1] for i in range(len(dataset))]\n",
    "standardizer = Standardizer(torch.Tensor(outputs))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "## Train model\n",
    "loss = []\n",
    "mae = []\n",
    "epoch = []\n",
    "for i in range(n_epochs):\n",
    "    epoch_loss, epoch_mae = train_model(\n",
    "        epoch=i,\n",
    "        model=model,\n",
    "        training_dataloader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        standardizer=standardizer,\n",
    "        use_GPU=use_GPU,\n",
    "        max_atoms=max_atoms,\n",
    "        node_vec_len=node_vec_len,\n",
    "    )\n",
    "    loss.append(epoch_loss)\n",
    "    mae.append(epoch_mae)\n",
    "    epoch.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test model\n",
    "\n",
    "test_loss, test_mae, test_rmse = test_model(model=model,\n",
    "                                 test_dataloader=test_loader,\n",
    "                                 loss_fn=loss_fn,\n",
    "                                 standardizer=standardizer,\n",
    "                                 use_GPU=use_GPU,\n",
    "                                 max_atoms=max_atoms,\n",
    "                                 node_vec_len=node_vec_len,\n",
    "                                 )\n",
    "\n",
    "# Print final results\n",
    "print(f\"Training Loss: {loss[-1]:.2f}\")\n",
    "print(f\"Training MAE: {mae[-1]:.2f}\")\n",
    "print(f\"Test Loss: {test_loss:.2f}\")\n",
    "print(f\"Test MAE: {test_mae:.2f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize using Ax Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seeds\n",
    "def set_seeds(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def train_test(parametrization):\n",
    "    ## Inputs\n",
    "    max_atoms = 100\n",
    "    node_vec_len = 60\n",
    "    train_size = 0.8\n",
    "    batch_size = 32\n",
    "    hidden_nodes = parametrization['hidden_nodes'] # Default = 60\n",
    "    n_conv_layers = parametrization['n_conv_layers'] # Default = 4\n",
    "    n_hidden_layers = parametrization['n_hidden_layers'] # Default = 2\n",
    "    learning_rate = parametrization['learning_rate'] # Default = 0.01\n",
    "    n_epochs = 100\n",
    "    use_GPU = True\n",
    "\n",
    "    ## Create dataset\n",
    "    main_path = Path().resolve().parent\n",
    "    data_path = main_path / 'data' / 'train.csv'\n",
    "    dataset = GraphDataset(dataset_path=data_path,\n",
    "                        max_atoms=max_atoms,\n",
    "                        node_vec_len=node_vec_len)\n",
    "\n",
    "    ## Split data into train and test\n",
    "    # Get sizes\n",
    "    dataset_indices = np.arange(0, len(dataset), 1)\n",
    "    train_size = int(np.round(train_size * len(dataset)))\n",
    "    test_size = len(dataset) - train_size\n",
    "\n",
    "    # Randomly sample train and test indices\n",
    "    train_indices = np.random.choice(dataset_indices,\n",
    "                                    size=train_size,\n",
    "                                    replace=False)\n",
    "    test_indices = np.array(list(set(dataset_indices) - set(train_indices)))\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "    train_loader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            sampler=train_sampler,\n",
    "                            collate_fn=collate_graph_dataset)\n",
    "    test_loader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            sampler=test_sampler,\n",
    "                            collate_fn=collate_graph_dataset)\n",
    "\n",
    "    ## Initialize model, standardizer, optimizer and loss functions\n",
    "    # Model\n",
    "    model = ChemGCN(node_vec_len=node_vec_len,\n",
    "                    node_fea_len=hidden_nodes,\n",
    "                    hidden_fea_len=hidden_nodes,\n",
    "                    n_conv=n_conv_layers,\n",
    "                    n_hidden=n_hidden_layers,\n",
    "                    n_outputs=1,\n",
    "                    p_dropout=0.1)\n",
    "    # Transfer to GPU\n",
    "    if use_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    # Standardizer\n",
    "    outputs = [dataset[i][1] for i in range(len(dataset))]\n",
    "    standardizer = Standardizer(torch.Tensor(outputs))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=learning_rate)\n",
    "\n",
    "    # Loss function\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    ## Train model\n",
    "    for i in range(n_epochs):\n",
    "        epoch_loss, epoch_mae = train_model(\n",
    "            epoch=i,\n",
    "            model=model,\n",
    "            training_dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            standardizer=standardizer,\n",
    "            use_GPU=use_GPU,\n",
    "            max_atoms=max_atoms,\n",
    "            node_vec_len=node_vec_len,\n",
    "        )\n",
    "\n",
    "    _, _, test_rmse = test_model(model=model,\n",
    "                                    test_dataloader=test_loader,\n",
    "                                    loss_fn=loss_fn,\n",
    "                                    standardizer=standardizer,\n",
    "                                    use_GPU=use_GPU,\n",
    "                                    max_atoms=max_atoms,\n",
    "                                    node_vec_len=node_vec_len,\n",
    "                                    )\n",
    "\n",
    "    return test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # max_atoms = 100\n",
    "    # node_vec_len = 60\n",
    "    # train_size = 0.7\n",
    "    # batch_size = 32\n",
    "    # hidden_nodes = 60\n",
    "    # n_conv_layers = 4\n",
    "    # n_hidden_layers = 2\n",
    "    # learning_rate = 0.01\n",
    "    # n_epochs = 100\n",
    "\n",
    "    # hidden_nodes = parametrization['hidden_nodes'] # Default = 60\n",
    "    # n_conv_layers = parametrization['n_conv_layers'] # Default = 4\n",
    "    # n_hidden_layers = parametrization['n_hidden_layers'] # Default = 2\n",
    "    # learning_rate = parametrization['learning_rate'] # Default = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "from ax.service.utils.report_utils import exp_to_df\n",
    "from ax.utils.notebook.plotting import init_notebook_plotting, render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client = AxClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an experiment with required arguments: name, parameters, and objective_name.\n",
    "ax_client.create_experiment(\n",
    "    name=\"GCN_hyperparameter_search\",  # The name of the experiment.\n",
    "    parameters=[\n",
    "        {\n",
    "            \"name\": \"hidden_nodes\",  # The name of the parameter.\n",
    "            \"type\": \"range\",  # The type of the parameter (\"range\", \"choice\" or \"fixed\").\n",
    "            \"bounds\": [10, 100],  # The bounds for range parameters. \n",
    "            \"value_type\": \"int\",  # Optional, the value type (\"int\", \"float\", \"bool\" or \"str\"). Defaults to inference from type of \"bounds\".\n",
    "            \"log_scale\": False,  # Optional, whether to use a log scale for range parameters. Defaults to False.\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"n_conv_layers\",  \n",
    "            \"type\": \"range\",  \n",
    "            \"bounds\": [1, 10],\n",
    "            \"value_type\": \"int\" \n",
    "        },\n",
    "        {\n",
    "            \"name\": \"n_hidden_layers\",\n",
    "            \"type\": \"range\",\n",
    "            \"bounds\": [1, 10],\n",
    "            \"value_type\": \"int\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"learning_rate\",\n",
    "            \"type\": \"range\",\n",
    "            \"bounds\": [1e-5, 0.1],\n",
    "            \"value_type\": \"float\",\n",
    "            \"log_scale\": True,\n",
    "        },\n",
    "    ],\n",
    "    objectives={\"rmse\": ObjectiveProperties(minimize=True)},  # The objective name and minimization setting.\n",
    "    # parameter_constraints: Optional, a list of strings of form \"p1 >= p2\" or \"p1 + p2 <= some_bound\".\n",
    "    # outcome_constraints: Optional, a list of strings of form \"constrained_metric <= some_bound\".\n",
    "    overwrite_existing_experiment=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hidden_nodes = parametrization['hidden_nodes'] # Default = 60\n",
    "# n_conv_layers = parametrization['n_conv_layers'] # Default = 4\n",
    "# n_hidden_layers = parametrization['n_hidden_layers'] # Default = 2\n",
    "# learning_rate = parametrization['learning_rate'] # Default = 0.01\n",
    "\n",
    "# Create base trial\n",
    "base_params = {'hidden_nodes': 60,\n",
    "               'n_conv_layers': 4,\n",
    "               'n_hidden_layers': 2,\n",
    "               'learning_rate': 0.01}\n",
    "\n",
    "ax_client.attach_trial(\n",
    "    parameters=base_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_parameters = ax_client.get_trial_parameters(trial_index=0)\n",
    "ax_client.complete_trial(trial_index=0, raw_data=train_test(baseline_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    # Local evaluation here can be replaced with deployment to external system.\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=train_test(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.get_trials_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters, values = ax_client.get_best_parameters()\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(ax_client.get_feature_importances())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.get_trials_data_frame().to_csv(\"../data/optimization_results/GCN_optimization.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(ax_client.get_contour_plot(param_x=\"learning_rate\", param_y=\"n_conv_layers\", metric_name=\"rmse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(\n",
    "    ax_client.get_optimization_trace()\n",
    ")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
